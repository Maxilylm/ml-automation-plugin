---
name: team-coldstart
description: "Launch a full data workflow from raw data to deployed solution with interactive dashboard. Orchestrates multiple agents through analysis, processing, modeling, and deployment stages. Works with any tabular data."
user_invocable: true
aliases: ["team coldstart", "coldstart", "full-pipeline", "run-all"]
---

# Team Cold Start - Full Data Workflow Orchestration

You are launching a complete data workflow from raw data to deployed solution with an interactive Streamlit dashboard. This skill orchestrates multiple specialized agents through all stages of the data lifecycle.

## Overview

The `/team coldstart` command initiates an end-to-end data pipeline:
1. **Initialize** - Set up project, validate data, detect data type
2. **Analyze** - EDA, quality review, feature/column recommendations
3. **Process** - Build and validate data processing pipeline
4. **Model** - Train and compare models (if prediction task) OR generate insights (if analysis task)
5. **Evaluate** - Comprehensive evaluation with metrics and visualizations
6. **Dashboard** - Create interactive Streamlit dashboard with results
7. **Productionalize** - Create production-ready code and API
8. **Deploy** - Deploy to target environment (optional)
9. **Finalize** - Generate reports, merge PRs

## Usage

```bash
# Full pipeline with auto-detection
/team coldstart data/raw/sales.csv

# With specific target variable (ML task)
/team coldstart data/raw/customers.csv --target Churn

# Analysis-only mode (no ML, just insights + dashboard)
/team coldstart data/raw/survey.csv --mode analysis

# Skip deployment stage
/team coldstart data/raw/data.csv --no-deploy

# Deploy to specific target
/team coldstart data/raw/data.csv --deploy-to snowflake

# Custom dashboard title
/team coldstart data/raw/data.csv --dashboard-title "Sales Analytics"
```

## Your Orchestration Workflow

### Stage 1: Initialize

**Actions:**
1. Create feature branch: `feature/data-pipeline-{timestamp}`
2. Create task list for tracking progress
3. Validate input data exists and is readable
4. Auto-detect data type and task mode:
   - **ML Mode**: Target column specified or detectable (classification/regression)
   - **Analysis Mode**: No clear target, focus on insights and visualization
5. Set up project structure

**Tasks to Create:**
- [ ] Initialize project structure
- [ ] Validate input data quality
- [ ] Run exploratory data analysis
- [ ] Build data processing pipeline
- [ ] Train models OR generate insights
- [ ] Evaluate results
- [ ] Create Streamlit dashboard
- [ ] Create production code
- [ ] Deploy (if requested)
- [ ] Generate final report

**Output:**
```markdown
## Stage 1: Initialize ✓

- Branch: feature/data-pipeline-20240115-1030
- Data validated: data/raw/sales.csv (10000 rows, 25 columns)
- Detected mode: ML (target: Revenue) OR Analysis (no target)
- Quality check: PASSED (no critical issues)
```

### Stage 2: Analysis (Sequential → Parallel with Report Bus)

**Step 2a: Run EDA first** (generates report for downstream agents):

1. **eda-analyst** - Comprehensive EDA + Report Bus Output
   ```
   Perform thorough exploratory data analysis on {data_path}.
   Generate statistics, visualizations, and identify data quality issues.

   CRITICAL: Use ml_utils to save structured EDA report:
   - Copy ml_utils.py from the plugin templates if not present in src/
   - Call generate_eda_summary(df, target_col) and save_eda_report(summary)
   - Also save reports/eda_report.md with narrative findings
   - The save_eda_report function will automatically write to the report bus

   Output: EDA report + agent bus report (eda-analyst_report.json)
   ```

   After EDA completes, the data fingerprint is automatically generated by eda-analyst
   and stored in `.claude/mlops/data-versions/`. This enables downstream reproducibility.

**Step 2b: Run these in PARALLEL** (after EDA completes — they all read the EDA report independently):

Spawn ALL THREE agents concurrently using multiple Task tool calls in a single message:

2. **ml-theory-advisor** - Data quality and leakage review (if ML mode)
   ```
   Review the dataset {data_path} for potential data leakage risks.
   Check for features that may contain target information.
   FIRST: Read .claude/reports/eda-analyst_report.json (or .claude/eda_report.json) for prior EDA context.
   Validate data splitting strategy.
   WHEN DONE: Write your report using save_agent_report("ml-theory-advisor", {...})
   Output: Quality and leakage assessment report + agent bus report
   ```

3. **feature-engineering-analyst** - ML Feature Engineering
   ```
   Analyze {data_path} and engineer predictive features.
   FIRST: Read .claude/reports/eda-analyst_report.json (or .claude/eda_report.json) — use the EDA findings
   to inform your feature engineering strategy.
   WHEN DONE: Write your report using save_agent_report("feature-engineering-analyst", {...})
   Output: Feature engineering plan + agent bus report
   ```

   After feature engineering completes, all features are registered in `.claude/mlops/feature-store.json`
   by feature-engineering-analyst. This enables feature reuse and lineage tracking.

4. **frontend-ux-analyst** - Dashboard Planning
   ```
   Review the EDA findings for {data_path} to plan dashboard visualizations.
   FIRST: Read .claude/reports/eda-analyst_report.json for data context.
   Recommend dashboard layout, key visualizations, and interactive features.
   WHEN DONE: Write your report using save_agent_report("frontend-ux-analyst", {...})
   Output: Dashboard design recommendations + agent bus report
   ```

**Wait for all three to complete before proceeding to Stage 3.**

**Output:**
```markdown
## Stage 2: Analysis ✓

### EDA Summary
- Dataset: {rows} rows x {cols} columns
- Target: {target} ({type}) OR None (analysis mode)
- Key columns: {list}
- Missing values: {count} columns with >5% missing
- Recommendations: {list}

### Parallel Analysis Results
- Feature Engineering: {count} features recommended
- ML Theory: {leakage_status}, {methodology_notes}
- Dashboard Planning: {visualization_recommendations}
```

### Stage 2c: Gate 1 — Reflect on Feature Engineering (Reflection Loop)

**Reflection gate** — validates feature engineering output before preprocessing proceeds.

```
iteration = 0
max_iterations = {--max-reflect, default: 2}
```

1. **Spawn ml-theory-advisor in reflection mode:**
   ```
   REFLECTION MODE — Gate 1: Post-Feature-Engineering

   Read ALL reports in .claude/reports/, especially:
   - eda-analyst_report.json
   - feature-engineering-analyst_report.json

   Evaluate whether the proposed feature engineering strategy is sound:
   - Are features appropriate for the problem domain?
   - Are domain-specific transformations included (e.g., adstock for MMM, lag for time series)?
   - Any leakage risk in the proposed features?
   - Is variable selection justified?

   Write reflection report using save_reflection_report("post-feature-engineering", {...})
   Your verdict must be either "approved" or "revise" with specific corrections.
   ```

2. **Read reflection report verdict:**
   - If `"verdict": "approved"` → proceed to Stage 3
   - If `"verdict": "revise"` → re-spawn feature-engineering-analyst with corrections:
     ```
     REVISION REQUEST from ml-theory-advisor reflection gate.

     Read the reflection report at .claude/reports/ml-theory-advisor_reflection_post-feature-engineering_report.json
     Address the corrections listed. Update your feature engineering plan accordingly.
     Write your updated report using save_agent_report("feature-engineering-analyst", {...})
     ```
   - Increment iteration, loop back to step 1

3. **If max iterations reached with verdict still "revise":**
   - Log warning: "Gate 1: Max reflection iterations reached, proceeding with best effort"
   - Proceed to Stage 3

### Stage 3: Data Processing

**Actions:**
1. Build data processing pipeline based on EDA findings
2. Handle missing values, encoding, scaling as needed
3. Invoke `ml-theory-advisor` to validate no leakage (if ML mode)
4. Generate unit tests for processing functions
5. Create PR for processing code
6. Invoke `brutal-code-reviewer` for code review
7. Merge PR after approval

**Code to generate:**
```python
# First, ensure ml_utils.py is in the project (copy from plugin templates if not present)
# src/ml_utils.py provides: load_data, detect_column_types, build_preprocessor, safe_split, etc.

# src/processing.py
from src.ml_utils import detect_column_types, build_preprocessor, load_eda_report

def process_data(df, target_col=None, mode='ml'):
    """Process raw data for modeling or analysis.

    Uses prior EDA report if available to inform preprocessing decisions.
    """
    # Load EDA context if available
    eda_report = load_eda_report()

    # Detect column types (or use EDA report's classification)
    if eda_report and "column_types" in eda_report:
        col_types = eda_report["column_types"]
    else:
        col_types = detect_column_types(df, target_col=target_col)

    # Build preprocessor
    preprocessor = build_preprocessor(col_types["numerical"], col_types["categorical"])

    return preprocessor, col_types
```

**Output:**
```markdown
## Stage 3: Data Processing ✓

- Pipeline created: src/processing.py
- Tests generated: tests/unit/test_processing.py
- Coverage: 92%
- PR #1: Merged ✓
- Quality check: PASSED
```

### Stage 3b: Gate 2 — Reflect on Preprocessing Pipeline (Reflection Loop)

**Reflection gate** — validates preprocessing pipeline before training proceeds.

Same iteration loop pattern as Gate 1, but:
- Gate name: `"post-preprocessing"`
- Reflection prompt focuses on: pipeline design, scaling/encoding choices, data flow correctness, consistency with chosen model family
- Reports to read: preprocessing report + feature-engineering report + EDA report
- On revise: re-run Stage 3 (preprocessing) with corrections from reflection report

### Stage 4: Modeling / Insights Generation

**For ML Mode:**
1. Train baseline model (appropriate for task type)
2. Train advanced models (ensemble methods)
3. Invoke `ml-theory-advisor` to review methodology
4. Compare and select best model

**For Analysis Mode:**
1. Generate statistical summaries
2. Create aggregations and pivots
3. Identify key insights and patterns
4. Generate insight report

**Code to generate:**
```python
# src/model.py (ML mode)
def train_model(X, y, model_type='auto'):
    """Train and return best model for the task."""
    ...

# src/insights.py (Analysis mode)
def generate_insights(df, dimensions, metrics):
    """Generate key insights from data."""
    ...
```

**Output (ML Mode):**
```markdown
## Stage 4: Modeling ✓

### Models Trained
| Model | Metric 1 | Metric 2 | Metric 3 |
|-------|----------|----------|----------|
| Baseline | 0.79 | 0.76 | 0.74 |
| Advanced 1 | 0.84 | 0.82 | 0.80 |
| Advanced 2 | 0.85 | 0.83 | 0.81 |

Best model: Advanced 2
PR #2: Merged ✓
```

   After training completes, the developer agent logs the experiment to `.claude/mlops/experiments/`
   including rationale from prior agent reports, hyperparameters, and metrics across all splits.

**Output (Analysis Mode):**
```markdown
## Stage 4: Insights ✓

### Key Findings
1. Revenue increased 23% YoY
2. Top 3 categories drive 65% of sales
3. Regional variance identified

Insights saved: reports/insights.md
PR #2: Merged ✓
```

### Stage 4b: Gate 3 — Reflect on Training Approach (Reflection Loop)

**Reflection gate** — validates training approach before evaluation proceeds.

Same iteration loop pattern as Gate 1, but:
- Gate name: `"post-training"`
- Reflection prompt focuses on: model family appropriateness for the domain, hyperparameter strategy, validation approach, whether better alternatives exist
- Reports to read: ALL prior reports + training report
- On revise: re-run Stage 4 (training) with corrections from reflection report
- Example: For MMM, might recommend Bayesian regression over gradient boosting

### Stage 5: Evaluation

**For ML Mode:**
1. Run comprehensive model evaluation
2. Generate visualizations (confusion matrix, ROC, feature importance)
3. Invoke `ml-theory-advisor` to validate evaluation
4. Generate evaluation report

**For Analysis Mode:**
1. Validate insight accuracy
2. Generate supporting visualizations
3. Cross-check findings with data
4. Generate analysis report

**Output:**
```markdown
## Stage 5: Evaluation ✓

### Performance Summary (ML) / Validation Summary (Analysis)
- Key metrics validated
- Visualizations generated
- Cross-validation complete (ML)
- Insights verified (Analysis)

Methodology validated ✓
```

### Stage 5b: Post-Training Review (PARALLEL)

Spawn ALL THREE review agents concurrently:

1. **brutal-code-reviewer** - Code quality review
   ```
   Review all code written in this workflow for quality and maintainability.
   FIRST: Read all reports in .claude/reports/ for context on what was built.
   WHEN DONE: Write your report using save_agent_report("brutal-code-reviewer", {...})
   Output: Code review report + agent bus report
   ```

2. **ml-theory-advisor** - Methodology validation
   ```
   Review the trained model and evaluation results for methodology issues.
   FIRST: Read all reports in .claude/reports/ for full workflow context.
   WHEN DONE: Write your report using save_agent_report("ml-theory-advisor", {...})
   Output: Theory review report + agent bus report
   ```

3. **frontend-ux-analyst** - Dashboard UX review (if dashboard exists)
   ```
   Review the generated Streamlit dashboard for UX quality.
   FIRST: Read all reports in .claude/reports/ for context.
   WHEN DONE: Write your report using save_agent_report("frontend-ux-analyst", {...})
   Output: UX review report + agent bus report
   ```

**Wait for all three to complete before proceeding to Stage 5c.**

### Stage 5c: MLOps Registry Validation

**mlops-engineer** validates that all MLOps registries are complete:

1. **Spawn mlops-engineer for registry validation:**
   ```
   MLOps Registry Validation — verify all registries are complete.

   Read all reports in .claude/reports/ for context.
   Read all registries in .claude/mlops/ (model-registry.json, feature-store.json, experiments/, data-versions/).

   Validate:
   1. Model registered with correct task_type and metrics
   2. All features cataloged with transformations and lineage
   3. Experiment logged with rationale from prior agent reports
   4. Data fingerprint exists
   5. Lineage chain is complete: data -> features -> experiment -> model

   If any registry is incomplete, fill gaps from available reports.
   Compare new model metrics with champion (if exists) and promote if better.

   Write your report using save_agent_report("mlops-engineer", {...})
   ```

2. **Output:**
   ```markdown
   ## Stage 5c: MLOps Registry Validation ✓

   - Model registered: model_20260223_143022 (challenger -> champion)
   - Features cataloged: 12 features in feature-store.json
   - Experiment logged: exp_20260223_143022
   - Data fingerprinted: sha256:abc123...
   - Lineage: complete ✓
   ```

### Stage 6: Streamlit Dashboard

**Actions:**
1. Create interactive Streamlit dashboard with all results
2. Include visualizations from EDA and evaluation
3. Add interactive filters and controls
4. Include model predictions (ML) or data explorer (Analysis)
5. Generate dashboard code

**Dashboard Components:**

```python
# dashboard/app.py
import streamlit as st
import pandas as pd
import plotly.express as px

st.set_page_config(page_title="{Project} Dashboard", layout="wide")

# Sidebar filters
st.sidebar.header("Filters")

# Main content tabs
tab1, tab2, tab3, tab4 = st.tabs(["Overview", "Analysis", "Results", "Predictions"])

with tab1:
    # KPI metrics
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Total Records", "{count}")
    col2.metric("Key Metric 1", "{value}")
    col3.metric("Key Metric 2", "{value}")
    col4.metric("Model Accuracy", "{accuracy}%")  # ML mode only

    # Overview charts
    st.plotly_chart(fig_overview)

with tab2:
    # EDA visualizations
    st.subheader("Data Distribution")
    st.plotly_chart(fig_distributions)

    st.subheader("Correlations")
    st.plotly_chart(fig_correlations)

with tab3:
    # Results (ML: model performance / Analysis: key insights)
    st.subheader("Key Findings")
    # Feature importance or insight cards

with tab4:
    # Interactive predictions (ML) or data explorer (Analysis)
    st.subheader("Make Predictions" if ml_mode else "Explore Data")
    # Input form for predictions or data filters
```

**Output:**
```markdown
## Stage 6: Streamlit Dashboard ✓

- Dashboard created: dashboard/app.py
- Components: Overview, Analysis, Results, Predictions
- Visualizations: 8 interactive charts
- Filters: Date range, Category, Region

### To Run Dashboard:
```bash
streamlit run dashboard/app.py
```

Dashboard URL: http://localhost:8501
PR #3: Merged ✓
```

### Stage 7: Productionalization

**Actions:**
1. Invoke `mlops-engineer` to create production code
2. Create FastAPI endpoint (predictions for ML, data API for analysis)
3. Create Dockerfile with dashboard included
4. Generate integration tests
5. Create PR for production code
6. Invoke `brutal-code-reviewer` for review
7. Merge after approval

**Files to generate:**
- `api/app.py` - FastAPI application
- `api/schemas.py` - Pydantic models
- `dashboard/app.py` - Streamlit dashboard
- `Dockerfile` - Container configuration
- `docker-compose.yml` - Service orchestration (API + Dashboard)

**Output:**
```markdown
## Stage 7: Productionalization ✓

- API created: api/app.py
- Dashboard created: dashboard/app.py
- Docker configured: Dockerfile
- Integration tests: 85% coverage
- PR #4: Merged ✓
- Production checklist: PASSED
```

### Stage 8: Deployment (Optional)

**If deployment requested:**

1. Determine target (local, snowflake, streamlit-sis, aws, gcp)
2. Invoke appropriate agent:
   - **Local**: `mlops-engineer` - Docker deployment
   - **Snowflake**: Deploy to Snowflake + Streamlit in Snowflake
   - **Cloud**: AWS/GCP deployment

3. Deploy both API and Dashboard
4. Verify health checks

**Output:**
```markdown
## Stage 8: Deployment ✓

- Target: local (or snowflake/aws/gcp)
- API deployed: http://localhost:8000
- Dashboard deployed: http://localhost:8501
- Health check: PASSED
```

### Stage 9: Finalization

**Actions:**
1. Generate comprehensive final report
2. Ensure all PRs merged to main
3. Update documentation
4. Close tasks
5. Output dashboard URL and access instructions

**Final Report:**
```markdown
## Project Complete: {Project Name}

### Summary
- Duration: {time}
- Mode: ML / Analysis
- PRs Merged: {count}
- Test Coverage: {percentage}%
- Key Metric: {value}

### Artifacts
- Data Pipeline: src/processing.py
- Model/Insights: src/model.py or src/insights.py
- API: http://localhost:8000
- Dashboard: http://localhost:8501

### Files Created
- src/processing.py
- src/model.py (ML) or src/insights.py (Analysis)
- api/app.py
- dashboard/app.py
- tests/unit/test_*.py
- tests/integration/test_api.py
- Dockerfile
- docker-compose.yml

### Dashboard Access
```bash
# Local development
streamlit run dashboard/app.py

# Docker
docker-compose up
# Dashboard: http://localhost:8501
# API: http://localhost:8000
```

### Next Steps
1. Review dashboard and customize visualizations
2. Share dashboard URL with stakeholders
3. Set up monitoring and alerts
4. Plan for data refresh schedule
```

## Error Handling

If any stage fails:
1. Log the error with context
2. Attempt automatic remediation if possible
3. Create an issue for manual intervention
4. Continue with remaining independent stages
5. Report partial completion status

## Configuration Options

| Option | Default | Description |
|--------|---------|-------------|
| `--target` | Auto-detect | Target variable name (triggers ML mode) |
| `--mode` | auto | Force mode: `ml` or `analysis` |
| `--no-deploy` | false | Skip deployment stage |
| `--deploy-to` | local | Deployment target (local, snowflake, aws, gcp) |
| `--dashboard-title` | Auto | Custom title for Streamlit dashboard |
| `--test-size` | 0.2 | Test split ratio (ML mode) |
| `--cv-folds` | 5 | Cross-validation folds (ML mode) |
| `--coverage-threshold` | 80 | Minimum test coverage |
| `--no-dashboard` | false | Skip dashboard creation |
| `--max-reflect` | 2 | Maximum reflection iterations per gate (0 to skip gates) |

## Dashboard Customization

The generated Streamlit dashboard includes:
- **Overview Tab**: KPI metrics, summary statistics
- **Analysis Tab**: EDA visualizations, distributions, correlations
- **Results Tab**: Model performance (ML) or key insights (Analysis)
- **Interactive Tab**: Predictions (ML) or data explorer (Analysis)

To customize after generation:
1. Edit `dashboard/app.py`
2. Add custom visualizations to `dashboard/components/`
3. Modify filters in sidebar
4. Run `streamlit run dashboard/app.py` to preview

## Agent Coordination

Throughout the workflow, agents are coordinated to:
- Track progress via task list
- Coordinate handoffs between stages
- Handle blockers and escalations
- Generate status updates

Invoke agents using the Task tool with appropriate prompts and context from previous stages.

## Output Structure

```
project/
├── src/
│   ├── processing.py      # Data processing pipeline
│   ├── model.py           # ML model (if ML mode)
│   └── insights.py        # Insights generation (if Analysis mode)
├── api/
│   ├── app.py             # FastAPI application
│   └── schemas.py         # Pydantic models
├── dashboard/
│   ├── app.py             # Streamlit dashboard
│   └── components/        # Reusable dashboard components
├── tests/
│   ├── unit/
│   └── integration/
├── reports/
│   ├── eda_report.md
│   └── final_report.md
├── models/                # Saved models (ML mode)
├── Dockerfile
├── docker-compose.yml
└── requirements.txt
```
