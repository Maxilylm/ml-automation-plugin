# v1.4.0 Implementation Plan: Iterative Workflows, Pre-Stage Reflection, Lessons Learned

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add three features to the ml-automation plugin: iterative self-check loops for all stages, pre-stage reflection planning, and a persistent lessons learned system.

**Architecture:** All three features share a common flow: load lessons → reflect → execute → validate → iterate → record lessons. New utility functions go into `templates/ml_utils.py`. Workflow orchestration updates go into `commands/team-coldstart.md` and `commands/team-analyze.md`. A new hook handles stage validation.

**Tech Stack:** Python (ml_utils.py), Markdown (command definitions), Bash (hooks), JSON (data formats)

---

## Task 1: Create Feature Branch

**Files:** None (git operations only)

**Step 1: Create and switch to feature branch**

Run: `git checkout -b feature/v1.4-iterative-reflection-lessons`
Expected: Branch created and switched

**Step 2: Verify branch**

Run: `git branch --show-current`
Expected: `feature/v1.4-iterative-reflection-lessons`

---

## Task 2: Add Lessons Learned System to ml_utils.py

**Files:**
- Modify: `templates/ml_utils.py` (append after line 913, end of file)

**Why first:** The lessons system is consumed by both other features (iterative checks save lessons, pre-stage reflection reads lessons). Build the foundation first.

**Step 1: Add the lessons learned section**

Append to `templates/ml_utils.py` after the last line:

```python


# =============================================================================
# 10. LESSONS LEARNED
# =============================================================================

LESSONS_FILENAME = "lessons-learned.json"
LESSONS_SCHEMA_VERSION = "1.0"

PLATFORM_LESSONS_DIRS = [
    ".claude",
    ".cursor",
    ".codex",
    ".opencode",
]


def save_lesson(lesson_data, output_dirs=None):
    """
    Record a lesson learned. If a similar lesson exists (same stage + title substring match),
    increment times_encountered instead of creating a duplicate.

    Args:
        lesson_data: Dict with keys: stage, category (mistake|solution|pattern|tip),
            severity (high|medium|low), title, description, trigger, resolution,
            tags (list), applicable_to (list of command names)
    """
    from datetime import datetime, timezone

    if output_dirs is None:
        output_dirs = PLATFORM_LESSONS_DIRS

    now = datetime.now(timezone.utc)
    store = _load_lessons_store(output_dirs)

    title = lesson_data.get("title", "")
    stage = lesson_data.get("stage", "")

    # Deduplication: find existing lesson with same stage and overlapping title
    existing_idx = None
    for i, lesson in enumerate(store["lessons"]):
        if lesson.get("stage") == stage and (
            title.lower() in lesson.get("title", "").lower()
            or lesson.get("title", "").lower() in title.lower()
        ):
            existing_idx = i
            break

    if existing_idx is not None:
        store["lessons"][existing_idx]["times_encountered"] += 1
        store["lessons"][existing_idx]["last_encountered"] = now.isoformat()
        if lesson_data.get("resolution"):
            store["lessons"][existing_idx]["resolution"] = lesson_data["resolution"]
    else:
        lesson_id = f"lesson_{now.strftime('%Y%m%d_%H%M%S')}"
        entry = {
            "lesson_id": lesson_id,
            "created_at": now.isoformat(),
            "stage": stage,
            "category": lesson_data.get("category", "tip"),
            "severity": lesson_data.get("severity", "medium"),
            "title": title,
            "description": lesson_data.get("description", ""),
            "trigger": lesson_data.get("trigger", ""),
            "resolution": lesson_data.get("resolution", ""),
            "tags": lesson_data.get("tags", []),
            "times_encountered": 1,
            "last_encountered": now.isoformat(),
            "applicable_to": lesson_data.get("applicable_to", []),
        }
        store["lessons"].append(entry)

    _save_lessons_store(store, output_dirs)
    return store


def load_lessons(search_dirs=None):
    """Load all lessons from the lessons-learned store. Returns list of lesson dicts."""
    store = _load_lessons_store(search_dirs)
    return store.get("lessons", [])


def get_relevant_lessons(stage=None, tags=None, search_dirs=None):
    """
    Get lessons relevant to a specific stage or tags.
    Returns list sorted by severity (high first) then recency.
    """
    lessons = load_lessons(search_dirs)

    if stage:
        lessons = [l for l in lessons if l.get("stage") == stage or stage in l.get("tags", [])]

    if tags:
        tag_set = set(tags)
        lessons = [l for l in lessons if tag_set & set(l.get("tags", []))]

    severity_order = {"high": 0, "medium": 1, "low": 2}
    lessons.sort(key=lambda l: (
        severity_order.get(l.get("severity", "low"), 2),
        -(l.get("times_encountered", 0)),
    ))

    return lessons


def format_lessons_for_prompt(lessons, max_lessons=5):
    """Format lessons as a string suitable for including in agent prompts."""
    if not lessons:
        return ""

    lines = ["LESSONS FROM PRIOR RUNS (avoid these mistakes, follow these patterns):"]
    for lesson in lessons[:max_lessons]:
        category = lesson.get("category", "tip")
        title = lesson.get("title", "Unknown")
        resolution = lesson.get("resolution", "")
        times = lesson.get("times_encountered", 1)
        severity = lesson.get("severity", "medium")

        line = f"- [{severity.upper()}] ({category}) {title}"
        if resolution:
            line += f" → FIX: {resolution}"
        if times > 1:
            line += f" (encountered {times}x)"
        lines.append(line)

    return "\n".join(lines)


def _load_lessons_store(search_dirs=None):
    """Load the lessons store from the first directory that has it."""
    if search_dirs is None:
        search_dirs = PLATFORM_LESSONS_DIRS

    for d in search_dirs:
        path = os.path.join(d, LESSONS_FILENAME)
        if os.path.exists(path):
            try:
                with open(path) as f:
                    return json.load(f)
            except (json.JSONDecodeError, KeyError):
                continue

    return {"version": LESSONS_SCHEMA_VERSION, "lessons": []}


def _save_lessons_store(store, output_dirs=None):
    """Save the lessons store to all platform directories."""
    if output_dirs is None:
        output_dirs = PLATFORM_LESSONS_DIRS

    for d in output_dirs:
        os.makedirs(d, exist_ok=True)
        path = os.path.join(d, LESSONS_FILENAME)
        with open(path, "w") as f:
            json.dump(store, f, indent=2, default=str)
```

**Step 2: Commit**

```bash
git add templates/ml_utils.py
git commit -m "feat: add lessons learned system to ml_utils.py

Persistent knowledge base that records mistakes, solutions, and patterns.
Supports deduplication, severity-based sorting, and prompt formatting.
Multi-platform storage in .claude/, .cursor/, .codex/, .opencode/."
```

---

## Task 3: Add Stage Validation Functions to ml_utils.py

**Files:**
- Modify: `templates/ml_utils.py` (append after the lessons learned section)

**Step 1: Add the stage validation section**

Append to `templates/ml_utils.py`:

```python


# =============================================================================
# 11. STAGE VALIDATION (Iterative Self-Check)
# =============================================================================

STAGE_VALIDATORS = {
    "eda": "_validate_eda_output",
    "feature-engineering": "_validate_feature_engineering_output",
    "preprocessing": "_validate_preprocessing_output",
    "training": "_validate_training_output",
    "evaluation": "_validate_evaluation_output",
    "dashboard": "_validate_dashboard_output",
}


def validate_stage_output(stage, context=None):
    """
    Run stage-specific validation checks.

    Args:
        stage: Stage name (eda, feature-engineering, preprocessing, training, evaluation, dashboard)
        context: Optional dict with stage-specific context (e.g., file paths, data)

    Returns:
        tuple: (passed: bool, errors: list[str])
    """
    if context is None:
        context = {}

    validator_name = STAGE_VALIDATORS.get(stage)
    if validator_name is None:
        return True, []

    validator = globals().get(validator_name)
    if validator is None:
        return True, []

    return validator(context)


def _validate_eda_output(context):
    """Validate EDA stage output."""
    errors = []
    report = load_eda_report()

    if report is None:
        errors.append("EDA report not found in any report directory")
        return False, errors

    required_keys = ["shape", "column_types"]
    for key in required_keys:
        if key not in report:
            errors.append(f"EDA report missing required key: '{key}'")

    shape = report.get("shape", {})
    if not shape.get("rows") or not shape.get("cols"):
        errors.append("EDA report has empty or zero shape (rows/cols)")

    if not report.get("numerical_stats") and not report.get("categorical_stats"):
        errors.append("EDA report has no numerical or categorical statistics")

    return len(errors) == 0, errors


def _validate_feature_engineering_output(context):
    """Validate feature engineering output."""
    errors = []
    reports = load_agent_reports()
    fe_report = reports.get("feature-engineering-analyst")

    if fe_report is None:
        errors.append("Feature engineering report not found")
        return False, errors

    findings = fe_report.get("findings", {})
    details = findings.get("details", findings)

    if isinstance(details, dict):
        features = details.get("features", details.get("recommended_features", []))
        if not features:
            errors.append("No features recommended in feature engineering report")

    recs = fe_report.get("recommendations", [])
    if not recs and not details:
        errors.append("Feature engineering report has no recommendations or details")

    return len(errors) == 0, errors


def _validate_preprocessing_output(context):
    """Validate preprocessing stage output."""
    errors = []

    # Check processing.py exists
    processing_paths = ["src/processing.py", "processing.py"]
    found = any(os.path.exists(p) for p in processing_paths)
    if not found:
        errors.append("Processing pipeline file not found (expected src/processing.py)")

    # Check test file exists
    test_paths = ["tests/unit/test_processing.py", "tests/test_processing.py"]
    found_test = any(os.path.exists(p) for p in test_paths)
    if not found_test:
        errors.append("No test file found for processing pipeline")

    return len(errors) == 0, errors


def _validate_training_output(context):
    """Validate training stage output."""
    errors = []

    # Check model artifact exists
    model_paths = ["models/", "src/models/"]
    import glob as globmod
    model_files = []
    for mp in model_paths:
        model_files.extend(globmod.glob(os.path.join(mp, "*.joblib")))
        model_files.extend(globmod.glob(os.path.join(mp, "*.pkl")))
        model_files.extend(globmod.glob(os.path.join(mp, "*.pickle")))
    if not model_files:
        errors.append("No model artifact found (expected .joblib or .pkl in models/)")

    # Check experiment was logged
    experiments = load_experiments()
    if not experiments:
        errors.append("No experiment logged in MLOps registry")

    return len(errors) == 0, errors


def _validate_evaluation_output(context):
    """Validate evaluation stage output."""
    errors = []

    reports = load_agent_reports()
    has_eval = any("eval" in name.lower() or "theory" in name.lower() for name in reports)
    if not has_eval:
        # Check for evaluation files
        import glob as globmod
        eval_files = globmod.glob("reports/*eval*") + globmod.glob("reports/*performance*")
        if not eval_files:
            errors.append("No evaluation report or metrics file found")

    return len(errors) == 0, errors


def _validate_dashboard_output(context):
    """Validate dashboard output (supplements the post-dashboard hook)."""
    import ast
    import re

    errors = []
    dashboard_path = context.get("dashboard_path", "dashboard/app.py")

    if not os.path.exists(dashboard_path):
        errors.append(f"Dashboard file not found: {dashboard_path}")
        return False, errors

    with open(dashboard_path) as f:
        source = f.read()

    # Syntax check
    try:
        ast.parse(source)
    except SyntaxError as e:
        errors.append(f"Dashboard syntax error: {e}")

    # Placeholder check
    placeholders = re.findall(r'"\{[A-Za-z_][A-Za-z0-9_]*\}"', source)
    if placeholders:
        errors.append(f"Unresolved placeholders: {placeholders}")

    return len(errors) == 0, errors
```

**Step 2: Commit**

```bash
git add templates/ml_utils.py
git commit -m "feat: add stage validation functions to ml_utils.py

Deterministic validation for EDA, feature engineering, preprocessing,
training, evaluation, and dashboard stages. Each validator checks for
required artifacts and report completeness."
```

---

## Task 4: Add Pre-Stage Plan I/O to ml_utils.py

**Files:**
- Modify: `templates/ml_utils.py` (append after stage validation section)

**Step 1: Add pre-stage plan functions**

Append to `templates/ml_utils.py`:

```python


# =============================================================================
# 12. PRE-STAGE PLANS
# =============================================================================

def save_stage_plan(stage, plan_data, output_dirs=None):
    """
    Save a pre-stage plan to the report bus directories.

    Args:
        stage: Stage name (e.g., 'analysis', 'preprocessing', 'training')
        plan_data: Dict with keys: stage, stage_number, objectives (list),
            approach (str), risks (list), success_criteria (list),
            lessons_applied (list), context_from_prior_stages (dict)
    """
    from datetime import datetime, timezone

    if output_dirs is None:
        output_dirs = PLATFORM_REPORT_DIRS

    plan = {
        "stage": stage,
        "stage_number": plan_data.get("stage_number", 0),
        "created_at": datetime.now(timezone.utc).isoformat(),
        "objectives": plan_data.get("objectives", []),
        "approach": plan_data.get("approach", ""),
        "risks": plan_data.get("risks", []),
        "success_criteria": plan_data.get("success_criteria", []),
        "lessons_applied": plan_data.get("lessons_applied", []),
        "context_from_prior_stages": plan_data.get("context_from_prior_stages", {}),
    }

    filename = f"stage_plan_{stage}.json"
    paths_written = []

    for d in output_dirs:
        os.makedirs(d, exist_ok=True)
        path = os.path.join(d, filename)
        with open(path, "w") as f:
            json.dump(plan, f, indent=2, default=str)
        paths_written.append(path)

    return paths_written


def load_stage_plan(stage, search_dirs=None):
    """
    Load the pre-stage plan for a specific stage.

    Returns:
        dict with plan data, or None if not found
    """
    if search_dirs is None:
        search_dirs = PLATFORM_REPORT_DIRS

    filename = f"stage_plan_{stage}.json"

    for d in search_dirs:
        path = os.path.join(d, filename)
        if os.path.exists(path):
            try:
                with open(path) as f:
                    return json.load(f)
            except (json.JSONDecodeError, KeyError):
                continue

    return None
```

**Step 2: Commit**

```bash
git add templates/ml_utils.py
git commit -m "feat: add pre-stage plan I/O to ml_utils.py

Save and load stage plans for pre-execution reflection. Plans contain
objectives, approach, risks, success criteria, and applied lessons."
```

---

## Task 5: Create post-stage-check.sh Hook

**Files:**
- Create: `hooks/post-stage-check.sh`

**Step 1: Write the hook script**

```bash
#!/bin/bash
# Post-stage validation hook
# Runs stage-specific validation checks using ml_utils.py
# Usage: bash post-stage-check.sh <stage_name> [context_json]

set -e

STAGE="${1:-}"
CONTEXT="${2:-{}}"
TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')

if [ -z "$STAGE" ]; then
    echo "[Post-Stage Check] No stage specified, skipping"
    exit 0
fi

echo "[Post-Stage Check] Validating stage: $STAGE"

# Find ml_utils.py
ML_UTILS=""
for p in src/ml_utils.py ml_utils.py templates/ml_utils.py; do
    if [ -f "$p" ]; then
        ML_UTILS="$p"
        break
    fi
done

if [ -z "$ML_UTILS" ]; then
    echo "  ⚠ ml_utils.py not found, skipping validation"
    exit 0
fi

# Run validation
python3 -c "
import sys, json
sys.path.insert(0, '$(dirname $ML_UTILS)')
from ml_utils import validate_stage_output

context = json.loads('$CONTEXT')
passed, errors = validate_stage_output('$STAGE', context)

if passed:
    print('  ✓ Stage validation PASSED')
else:
    print('  ✗ Stage validation FAILED:')
    for err in errors:
        print(f'    - {err}')
    sys.exit(1)
"

RESULT=$?

# Log
mkdir -p .claude
echo "$TIMESTAMP - Stage check: $STAGE (result=$RESULT)" >> .claude/workflow.log 2>/dev/null || true

exit $RESULT
```

**Step 2: Make executable**

Run: `chmod +x hooks/post-stage-check.sh`

**Step 3: Commit**

```bash
git add hooks/post-stage-check.sh
git commit -m "feat: add post-stage-check.sh hook for iterative validation

Generic stage validation hook that uses ml_utils.validate_stage_output()
to run deterministic checks after each workflow stage."
```

---

## Task 6: Update hooks.json with Stage Check Hook

**Files:**
- Modify: `hooks/hooks.json`

**Step 1: Add SubagentStop hook for stage validation**

In `hooks/hooks.json`, add a new entry to the `SubagentStop` array. Insert before the catch-all `"*"` matcher (which should remain last). Add this entry at line 63, before the `"*"` matcher:

```json
      {
        "matcher": "developer",
        "hooks": [
          {
            "type": "command",
            "command": "bash ${CLAUDE_PLUGIN_ROOT}/hooks/post-stage-check.sh preprocessing"
          }
        ]
      },
```

Note: The full stage validation is orchestrated in the command files (team-coldstart.md) rather than hooks, since hooks can't know which stage just completed. The hook above catches the most common developer agent outputs. The main validation loop is in the workflow orchestration.

**Step 2: Commit**

```bash
git add hooks/hooks.json
git commit -m "feat: add developer SubagentStop hook for stage validation"
```

---

## Task 7: Update team-coldstart.md with Iterative + Reflection Flow

**Files:**
- Modify: `commands/team-coldstart.md`

This is the largest change. We need to add to each stage:
1. A pre-stage reflection step (spawns domain expert to plan)
2. A self-check step after agent execution (validates output, retries on failure)
3. Lessons integration (load before, save on failure)

**Step 1: Add new configuration options to the Configuration Options table**

Find the configuration table (near end of file) and add three new rows:

```markdown
| `--max-check` | 2 | Maximum self-check iterations per stage |
| `--no-pre-reflect` | false | Skip pre-stage reflection planning |
| `--no-lessons` | false | Skip lessons consultation |
```

**Step 2: Add the integrated stage flow documentation**

Insert a new section after the Overview section (after line ~24, before "### Stage 1: Initialize"). This section explains the new stage flow pattern:

````markdown
## Stage Flow Pattern (v1.4.0)

Every stage from 2 onwards follows this enhanced flow:

```
1. LOAD LESSONS — get_relevant_lessons(stage) from lessons-learned.json
2. PRE-STAGE REFLECTION — domain expert reads reports + lessons, produces stage plan
3. EXECUTE — agent runs the stage, reading the stage plan first
4. SELF-CHECK — validate_stage_output() runs deterministic checks
   └── If fails: re-spawn agent with error feedback (max --max-check iterations)
   └── On persistent failure: save_lesson() recording the issue
5. POST-STAGE REFLECTION GATE — (existing v1.2.1) ml-theory-advisor validates strategy
   └── If revise: save_lesson() recording the correction
6. PROCEED to next stage
```

### Pre-Stage Reflection Prompt Template

Before each stage, spawn the appropriate reflector agent:

```
PRE-STAGE REFLECTION — Stage {N}: {stage_name}

Read ALL reports in .claude/reports/ for context on prior stages.
Read .claude/lessons-learned.json for relevant lessons from past runs.

Produce a stage plan with:
1. OBJECTIVES: What this stage must accomplish
2. APPROACH: Recommended strategy based on prior findings
3. RISKS: Potential issues to watch for (from lessons + reports)
4. SUCCESS CRITERIA: How to verify the stage succeeded

Save your plan using save_stage_plan("{stage_name}", {...})
```

| Stage | Reflector Agent |
|-------|----------------|
| 2 (Analysis) | ml-theory-advisor |
| 3 (Processing) | ml-theory-advisor |
| 4 (Modeling) | ml-theory-advisor |
| 5 (Evaluation) | ml-theory-advisor |
| 6 (Dashboard) | frontend-ux-analyst |
| 7 (Production) | mlops-engineer |

### Self-Check Loop Template

After each stage's agent completes:

```
iteration = 0
max_iterations = {--max-check, default: 2}

Run validate_stage_output("{stage_name}")
while validation fails AND iteration < max_iterations:
    Re-spawn agent with:
      "SELF-CHECK FAILED (attempt {iteration+1}/{max_iterations}):
       {error_list}
       FIRST: Read the stage plan at .claude/reports/stage_plan_{stage_name}.json
       Fix the issues identified above and complete the stage."
    Run validate_stage_output() again
    iteration += 1

If still failing after max_iterations:
    save_lesson({
        "stage": "{stage_name}",
        "category": "mistake",
        "severity": "high",
        "title": "Stage {stage_name} failed validation after {max_iterations} retries",
        "description": "Errors: {error_list}",
        "trigger": "validate_stage_output() failure",
        "tags": ["{stage_name}", "validation-failure"]
    })
    Log warning and proceed with best effort
```

### Lessons Integration

Before each stage, add to agent spawn prompts:
```python
from ml_utils import get_relevant_lessons, format_lessons_for_prompt

lessons = get_relevant_lessons(stage="{stage_name}")
lessons_text = format_lessons_for_prompt(lessons)
# Append lessons_text to the agent's spawn prompt
```

When a post-stage reflection gate requests revision:
```python
from ml_utils import save_lesson

save_lesson({
    "stage": "{stage_name}",
    "category": "mistake",
    "severity": "medium",
    "title": "Reflection gate correction: {brief description}",
    "description": "{full correction details from reflection report}",
    "resolution": "{what the revised approach should be}",
    "tags": ["{stage_name}", "reflection-gate"],
    "applicable_to": ["team-coldstart"]
})
```
````

**Step 3: Update Stage 2 (Analysis) with pre-reflection**

Before "Step 2a: Run EDA first", add:

````markdown
#### Stage 2 Pre-Reflection (unless --no-pre-reflect)

Spawn **ml-theory-advisor** in pre-reflection mode:

```
PRE-STAGE REFLECTION — Stage 2: Analysis

Read any prior reports in .claude/reports/ and .claude/lessons-learned.json.
This is the first analysis stage. Plan what EDA should focus on based on:
- The data file characteristics (columns, size)
- Any lessons from prior workflow runs
- Known data quality patterns to check

Save your plan using save_stage_plan("analysis", {...})
```

After the plan is saved, proceed to Step 2a. Include in the eda-analyst prompt:
"FIRST: Read the stage plan at .claude/reports/stage_plan_analysis.json. Follow its objectives and watch for identified risks."
````

**Step 4: Add self-check after Step 2a (EDA)**

After the existing Step 2a eda-analyst section, add:

````markdown
#### Step 2a-check: EDA Self-Check

Run `validate_stage_output("eda")`. If validation fails, re-spawn eda-analyst with error feedback (max `--max-check` iterations). On persistent failure, `save_lesson()` documenting the issue.
````

**Step 5: Add self-check after Step 2b (parallel agents)**

After Step 2b-verify, add:

````markdown
#### Step 2b-check: Feature Engineering Self-Check

Run `validate_stage_output("feature-engineering")`. If validation fails, re-spawn feature-engineering-analyst with error feedback (max `--max-check` iterations).
````

**Step 6: Update Stage 3, 4, 5, 6, 7 similarly**

For each remaining stage, add:
1. A "Stage N Pre-Reflection" section before the main agent execution
2. A "Step Na-check" self-check section after agent execution

The pattern is identical — just change the stage name and reflector agent per the table above.

**Step 7: Add Stage 9 lessons summary**

In Stage 9 (Finalization), add to the final report output:

````markdown
### Lessons Learned
- Total lessons recorded: {count}
- New lessons this run: {new_count}
- Most common issues: {top_3_by_times_encountered}
````

**Step 8: Commit**

```bash
git add commands/team-coldstart.md
git commit -m "feat: add iterative self-check, pre-stage reflection, and lessons to team-coldstart

Every stage now follows: load lessons → reflect → execute → validate → iterate.
New config options: --max-check, --no-pre-reflect, --no-lessons."
```

---

## Task 8: Update team-analyze.md with Iterative + Reflection Flow

**Files:**
- Modify: `commands/team-analyze.md`

**Step 1: Add stage flow pattern and pre-reflection**

Add the same Stage Flow Pattern section as team-coldstart (abbreviated version for the lighter workflow). Add pre-reflection before Step 2 (EDA) using ml-theory-advisor, and self-checks after Steps 2, 3, and 4.

Add configuration options:
```markdown
| `--max-check` | 2 | Maximum self-check iterations per stage |
| `--no-pre-reflect` | false | Skip pre-stage reflection |
| `--no-lessons` | false | Skip lessons consultation |
```

**Step 2: Add self-check after EDA step**

After Step 2 (EDA), add:

````markdown
#### Step 2-check: EDA Self-Check

Run `validate_stage_output("eda")`. If validation fails, re-spawn eda-analyst with error feedback.
````

**Step 3: Add self-check after parallel analysis (Step 4b)**

After Step 4b verification, add:

````markdown
#### Step 4b-check: Feature Engineering Self-Check

Run `validate_stage_output("feature-engineering")`. If validation fails, re-spawn feature-engineering-analyst with error feedback.
````

**Step 4: Add lessons summary to Step 5 (Summary Report)**

Add lessons section to the summary report template.

**Step 5: Commit**

```bash
git add commands/team-analyze.md
git commit -m "feat: add iterative self-check, pre-stage reflection, and lessons to team-analyze

Same pattern as team-coldstart: load lessons → reflect → execute → validate."
```

---

## Task 9: Update status.md to Show Lessons Count

**Files:**
- Modify: `commands/status.md`

**Step 1: Add lessons section to status display**

After the "Cross-Agent Insights" section in the status format, add:

````markdown
### Lessons Learned ({count} total)
- High severity: {high_count}
- Most recent: {latest_lesson_title} ({last_encountered})
- Most recurring: {most_encountered_title} ({times}x)

Use `--lessons` flag for full lessons listing.
````

**Step 2: Add --lessons flag**

Add to configuration table:
```markdown
| `--lessons` | false | Show full lessons learned listing |
```

**Step 3: Commit**

```bash
git add commands/status.md
git commit -m "feat: add lessons count and --lessons flag to status command"
```

---

## Task 10: Bump Version to 1.4.0 in plugin.json

**Files:**
- Modify: `.claude-plugin/plugin.json`

**Step 1: Update version and description**

Change line 3 from `"version": "1.3.1"` to `"version": "1.4.0"`.

Update description to mention new features:
```json
"description": "End-to-end ML automation workflow for Claude Code. Includes 10 specialized agents with shared report bus, reflection gates, MLOps registry layer, iterative self-check loops, pre-stage reflection planning, lessons learned system, parallel execution groups, /status and /registry commands, reusable ML utilities, and hooks for quality gates."
```

Add new keywords: `"iterative"`, `"reflection"`, `"lessons-learned"`

**Step 2: Commit**

```bash
git add .claude-plugin/plugin.json
git commit -m "chore: bump version to 1.4.0"
```

---

## Task 11: Update README.md with v1.4.0 Changelog

**Files:**
- Modify: `README.md`

**Step 1: Add v1.4.0 section**

Insert after line 1 (after the title, before "## What's New in v1.2.0"), add a new "What's New in v1.4.0" section:

````markdown
## What's New in v1.4.0

### Iterative Self-Check Loops

Every workflow stage now validates its output before proceeding. If validation fails, the agent is re-spawned with error feedback (configurable max iterations). This generalizes the dashboard smoke test to ALL stages:

- **EDA**: Report completeness, required keys, non-empty statistics
- **Feature Engineering**: Features list populated, no duplicates, leakage assessed
- **Preprocessing**: Pipeline file exists, tests exist
- **Training**: Model artifact present, experiment logged
- **Evaluation**: Evaluation report complete
- **Dashboard**: Syntax, placeholders, imports (existing enhanced)

```bash
/team coldstart data.csv --max-check 3   # Allow 3 retry iterations
/team coldstart data.csv --max-check 0   # Skip self-checks
```

### Pre-Stage Reflection

Before each major stage, a domain expert agent plans the approach by reading all prior reports and lessons. The executing agent reads this plan before starting, leading to better first-attempt quality.

| Stage | Reflector |
|-------|-----------|
| Analysis | ml-theory-advisor |
| Processing | ml-theory-advisor |
| Modeling | ml-theory-advisor |
| Evaluation | ml-theory-advisor |
| Dashboard | frontend-ux-analyst |
| Production | mlops-engineer |

```bash
/team coldstart data.csv                    # With pre-reflection (default)
/team coldstart data.csv --no-pre-reflect   # Skip pre-stage reflection
```

### Lessons Learned System

Persistent knowledge base that records mistakes, solutions, and successful patterns. Lessons are:
- **Written** when self-checks fail, reflection gates request revision, or agents recover from errors
- **Consulted** before each stage in pre-reflection prompts and agent spawn prompts
- **Deduplicated** automatically (same stage + similar title → increment counter)
- **Persisted** across workflow runs in `.claude/lessons-learned.json`

```bash
/status --lessons   # View all recorded lessons
```

### New Utility Functions

Added to `ml_utils.py`:
- `save_lesson()`, `load_lessons()`, `get_relevant_lessons()`, `format_lessons_for_prompt()`
- `validate_stage_output()` with per-stage validators
- `save_stage_plan()`, `load_stage_plan()`
````

**Step 2: Commit**

```bash
git add README.md
git commit -m "docs: add v1.4.0 changelog to README"
```

---

## Task 12: Final Review and Summary Commit

**Step 1: Verify all files changed**

Run: `git log --oneline feature/v1.4-iterative-reflection-lessons --not main`
Expected: 8-10 commits covering all changes

**Step 2: Review diff against main**

Run: `git diff main --stat`
Expected changes in:
- `templates/ml_utils.py` (significant additions)
- `commands/team-coldstart.md` (moderate additions)
- `commands/team-analyze.md` (moderate additions)
- `commands/status.md` (small additions)
- `hooks/hooks.json` (small additions)
- `hooks/post-stage-check.sh` (new file)
- `.claude-plugin/plugin.json` (version bump)
- `README.md` (changelog)
- `docs/plans/` (design doc + this plan)

**Step 3: Verify no broken JSON**

Run: `python3 -c "import json; json.load(open('.claude-plugin/plugin.json')); json.load(open('hooks/hooks.json')); print('All JSON valid')"`
Expected: "All JSON valid"
