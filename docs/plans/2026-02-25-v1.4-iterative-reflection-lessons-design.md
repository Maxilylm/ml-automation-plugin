# v1.4 Design: Iterative Workflows, Pre-Stage Reflection, Lessons Learned

**Date:** 2026-02-25
**Version:** 1.4.0
**Author:** Claude (autonomous design, user away)

## Problem Statement

v1.3.x agents run stages once without self-verification, reflection gates exist only at 3 specific points, and there is no persistent memory of past mistakes/solutions across workflow runs. This leads to:

1. Agent outputs that silently contain errors (only dashboard has a smoke test)
2. No planning before each stage — agents dive straight into execution
3. Repeated mistakes across different workflow runs with no learning

## Feature 1: Iterative Self-Check Loops

### What It Does

After every agent completes a stage, run a validation step. If validation fails, re-run the agent with error feedback. This generalizes the dashboard smoke test pattern (v1.3.1) to ALL workflow stages.

### Design

Each stage gets a `verify` step with stage-specific checks:

| Stage | Validation Checks |
|-------|-------------------|
| EDA (Stage 2a) | Report has required keys, stats are non-empty, quality_issues populated |
| Feature Engineering (2b) | Features list non-empty, no duplicate feature_ids, leakage_risk assessed |
| Preprocessing (3) | Pipeline produces output, no NaN in transformed data, tests pass |
| Training (4) | Model artifact exists, metrics within reasonable bounds, experiment logged |
| Evaluation (5) | Evaluation report complete, visualizations generated |
| Dashboard (6) | ast.parse + placeholder + import check (existing) |

**Iteration pattern** (same for all stages):

```
iteration = 0
max_iterations = configurable (default: 2)

agent runs stage
run stage-specific validation
while validation fails AND iteration < max_iterations:
    re-spawn agent with: "SELF-CHECK FAILED: {errors}. Fix and retry."
    run validation again
    iteration += 1

if still failing after max_iterations:
    log warning, proceed with best effort
    save_lesson() documenting the failure
```

**New utility functions in ml_utils.py:**

```python
def validate_stage_output(stage, artifacts):
    """Run stage-specific validation checks. Returns (passed: bool, errors: list[str])."""

def run_iterative_stage(stage, spawn_fn, validate_fn, max_iterations=2):
    """Generic iterative loop: run agent, validate, retry on failure."""
```

**Impact on team-coldstart.md:** Each stage section gets a `verify` substep after the agent completes. The existing dashboard smoke test (Step 6b) becomes a specific instance of this general pattern.

### Alternatives Considered

- **A: Per-agent self-validation** (agent validates itself before reporting) — Rejected because agents can't objectively validate their own work.
- **B: Dedicated validator agent** — Rejected as overkill; validation checks are deterministic, not requiring LLM judgment.
- **C: Post-hook validation** (chosen) — Validation logic in ml_utils.py, called from the workflow orchestration. Simple, testable, deterministic.

## Feature 2: Pre-Stage Reflection (Plan Before Execute)

### What It Does

Before each major stage, spawn a brief reflection that reads all available context (prior reports, lessons learned, stage objectives) and produces a "stage plan" with objectives, approach, risks, and success criteria. The executing agent reads this plan before starting.

### Design

**New reflection type:** `pre-stage` (vs existing `post-stage` gates which validate after)

Pre-stage reflection runs before Stages 2 through 7. Stage 1 (Initialize) doesn't need it — it's the starting point.

**Pre-stage plan format:**

```json
{
  "stage": "preprocessing",
  "stage_number": 3,
  "objectives": ["Build sklearn pipeline for numerical + categorical columns"],
  "approach": "Use ColumnTransformer with imputation, scaling, encoding",
  "risks": ["High cardinality in 'city' column — consider target encoding"],
  "success_criteria": ["Pipeline transforms without error", "No NaN in output", "Tests pass"],
  "lessons_applied": ["lesson_001: Always check for date columns before building pipeline"],
  "context_from_prior_stages": {
    "eda_findings": "12 numerical, 3 categorical, 2 datetime columns",
    "feature_plan": "15 features proposed including 3 interaction terms"
  }
}
```

**Who runs pre-stage reflection:**

| Stage | Reflector | Rationale |
|-------|-----------|-----------|
| 2 (Analysis) | ml-theory-advisor | ML methodology expertise |
| 3 (Processing) | ml-theory-advisor | Pipeline design expertise |
| 4 (Modeling) | ml-theory-advisor | Model selection expertise |
| 5 (Evaluation) | ml-theory-advisor | Evaluation methodology |
| 6 (Dashboard) | frontend-ux-analyst | UX/visualization expertise |
| 7 (Production) | mlops-engineer | Deployment expertise |

**New utility functions:**

```python
def save_stage_plan(stage, plan_data, output_dirs=None):
    """Save a pre-stage plan to the report bus."""

def load_stage_plan(stage, search_dirs=None):
    """Load the pre-stage plan for a specific stage."""
```

**How agents consume the plan:**
The agent spawn prompt includes: "FIRST: Read the stage plan at .claude/reports/stage_plan_{stage}.json. Follow the objectives, watch for identified risks, verify success criteria when done."

**Interaction with existing post-stage reflection gates:**
- Pre-stage reflection: plan BEFORE execution (new in v1.4)
- Post-stage reflection gates: validate AFTER execution (existing from v1.2.1)
- Both remain. The flow is: reflect → execute → validate → iterate if needed

### Alternatives Considered

- **A: Single pre-flight agent for all stages** — Rejected because different stages need different domain expertise.
- **B: Inline reflection in agent prompts** — Rejected because it doesn't produce a shareable plan artifact.
- **C: Domain-expert pre-stage reflection** (chosen) — Each stage gets reflection from the most relevant expert agent. Plans are saved as artifacts for traceability.

## Feature 3: Lessons Learned System

### What It Does

A persistent knowledge base that records mistakes, solutions, and successful patterns. Consulted before each stage to avoid repeating errors. Grows across workflow runs.

### Design

**Storage:** `.claude/lessons-learned.json` (+ multi-platform dirs)

**Lesson format:**

```json
{
  "version": "1.0",
  "lessons": [
    {
      "lesson_id": "lesson_20260225_143022",
      "created_at": "2026-02-25T14:30:22Z",
      "stage": "preprocessing",
      "category": "mistake|solution|pattern|tip",
      "severity": "high|medium|low",
      "title": "Date columns cause pipeline failure if not excluded",
      "description": "ColumnTransformer fails on datetime columns. Must detect and exclude or convert to numeric features (day_of_week, month, etc.) before building pipeline.",
      "trigger": "TypeError in ColumnTransformer.fit_transform()",
      "resolution": "Use detect_column_types() to exclude datetime columns, create numeric date features instead",
      "tags": ["preprocessing", "datetime", "pipeline", "sklearn"],
      "times_encountered": 1,
      "last_encountered": "2026-02-25T14:30:22Z",
      "applicable_to": ["team-coldstart", "preprocess"]
    }
  ]
}
```

**New utility functions in ml_utils.py:**

```python
def save_lesson(lesson_data):
    """Record a lesson learned. Increments times_encountered if similar lesson exists."""

def load_lessons(search_dirs=None):
    """Load all lessons from the lessons-learned store."""

def get_relevant_lessons(stage=None, tags=None, search_dirs=None):
    """Get lessons relevant to a specific stage or tags. Returns list sorted by severity and recency."""

def format_lessons_for_prompt(lessons, max_lessons=5):
    """Format lessons as a string suitable for including in agent prompts."""
```

**When lessons are written:**

1. **On iterative check failure:** When a stage fails validation, the error and eventual fix are recorded as a lesson.
2. **On reflection gate revision:** When a post-stage reflection requests revision, the correction is recorded.
3. **On agent error recovery:** When an agent encounters and recovers from an error.
4. **On successful patterns:** When a stage completes cleanly on first try with good metrics, record the approach as a "pattern" lesson.

**When lessons are consulted:**

1. **Pre-stage reflection:** The reflector reads relevant lessons before producing the stage plan.
2. **Agent spawn prompts:** Relevant lessons are appended to agent prompts: "LESSONS FROM PRIOR RUNS: {formatted_lessons}"

**Deduplication:** When a new lesson matches an existing one (same stage + similar title via substring match), increment `times_encountered` and update `last_encountered` instead of creating a duplicate.

**Persistence:** Lessons file is NOT gitignored — it's part of the project knowledge. It persists across workflow runs and even across branches (checked into version control).

### Alternatives Considered

- **A: In-memory only (per-session)** — Rejected because lessons should persist across runs.
- **B: Auto-memory in .claude/memory/** — Rejected because this is Claude-specific; lessons should be multi-platform and structured.
- **C: Structured JSON store** (chosen) — Multi-platform, structured, queryable by stage/tags, version-controlled.

## Integration: How The Three Features Work Together

```
┌─────────────────────────────────────────────────┐
│                 Stage N Flow                     │
│                                                  │
│  1. Load relevant lessons (Feature 3)            │
│  2. Pre-stage reflection + plan (Feature 2)      │
│     └── Reflector reads lessons + prior reports  │
│     └── Produces stage plan with risks/criteria  │
│  3. Agent executes stage (existing)              │
│     └── Reads stage plan before starting         │
│     └── Reads relevant lessons in prompt         │
│  4. Self-check validation (Feature 1)            │
│     └── If fails → re-run with error feedback    │
│     └── If fails → save lesson (Feature 3)       │
│  5. Post-stage reflection gate (existing v1.2.1) │
│     └── If revise → save lesson (Feature 3)      │
│  6. Proceed to Stage N+1                         │
└─────────────────────────────────────────────────┘
```

## Files to Modify

| File | Change |
|------|--------|
| `templates/ml_utils.py` | Add validation functions, stage plan I/O, lessons learned I/O |
| `commands/team-coldstart.md` | Add pre-stage reflection + self-check loops to each stage |
| `commands/team-analyze.md` | Add pre-stage reflection + self-check to analysis stages |
| `hooks/hooks.json` | Add post-stage-check hook for validation |
| `hooks/post-stage-check.sh` | New hook for stage output validation |
| `.claude-plugin/plugin.json` | Bump version to 1.4.0, update description |
| `README.md` | Document new features |
| `commands/status.md` | Show lessons count in status output |

## New Files

| File | Purpose |
|------|---------|
| `hooks/post-stage-check.sh` | Generic stage validation hook |

## Configuration Options (team-coldstart)

| Option | Default | Description |
|--------|---------|-------------|
| `--max-check` | 2 | Max self-check iterations per stage |
| `--no-pre-reflect` | false | Skip pre-stage reflection |
| `--no-lessons` | false | Skip lessons consultation |

## Version Bump

- `1.3.1` → `1.4.0` (minor: three new features, no breaking changes)

## Success Criteria

1. Every stage in team-coldstart has a pre-reflection step and post-validation step
2. Lessons file is created and populated during workflow runs
3. Lessons are consulted in pre-stage reflection prompts
4. Self-check loops catch and fix common errors (tested with intentional failures)
5. All existing tests still pass
6. README documents the new features
